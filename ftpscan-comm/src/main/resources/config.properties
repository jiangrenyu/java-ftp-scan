#要扫描的目录组，多个目录组以逗号分隔
groupIds=g001_xyl

#ftp服务器是否是sftp
isSFTP=true

#ftp登录超时时间,单位秒
loginTimeout=30

#数据库批查询大小
queryBatchSize=1000

#是否限制扫描任务的线程池大小,限制则需设置线程池的最大值
isThreadPoolLimit=true

#线程池的最大值
maxPoolSize=10

#扫描完一次所需要的最大时间,单位秒
maxTaskTime=120

#扫描间隔时间，单位秒
scanInternal=30

#账期格式
scan.dir.OptimesFormat=
#格式为：yyyyMMdd，yyyy-MM-dd

#账期小时格式
scan.dir.nowOptimeFormat=
#请输入相应格式为：yyyyMMddHH，yyyyMMdd/HH

#日志目录
logDir=log4j.properties

#############################kafka相关配置########################################

#生产者相关配置
serializer.class=kafka.serializer.StringEncoder

#kafka接收消息的 broker 地址列表，必填
metadata.broker.list=hadoop53:9099,hadoop54:9099,hadoop55:9099
bootstrap.servers=hadoop55:9099,hadoop54:9099
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer

#控制一个produce请求怎样才能算完成，0无需等待来自broker的ack，1得到leader 的ack，-1所有的ISR都接收到数据
request.required.acks=-1

#partition实现类
partitioner.class=com.bonc.ftputil.bean.FtpPartitioner

